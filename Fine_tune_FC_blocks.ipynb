{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine_tune_FC_blocks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgejQO6TfHJc"
      },
      "source": [
        "\"\"\"\n",
        "This code evaluates the accuracy of the base ResNet18 model on different classes of pulses and then only trains fc blocks on these classes.\n",
        "The FC blocks corrsponding to different classes are then concatenated together and saved in a dataframe \n",
        "\"\"\"\n",
        "import itertools\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import random\n",
        "import gzip, struct, time, itertools\n",
        "from torchvision import datasets\n",
        "import argparse\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import itertools\n",
        "import sys\n",
        "import time\n",
        "%matplotlib inline\n",
        "for p in range(0,10):\n",
        "    def load_pretrained(dict_path):\n",
        "        \"\"\"\n",
        "        Load a ResNet-18 model from `torchvision.models` without pre-trained weights. \n",
        "        \"\"\"\n",
        "        Resnet18 = models.resnet18(pretrained=False)\n",
        "        if (dict_path != None):\n",
        "            Resnet18.load_state_dict(torch.load(dict_path))\n",
        "        for param in Resnet18.parameters():\n",
        "            param.requires_grad = True\n",
        "        n_inputs = Resnet18.fc.in_features\n",
        "        Resnet18.fc = torch.nn.Linear(512,1000)\n",
        "        return Resnet18\n",
        "\n",
        "    def clear_checkpoint(checkpoint_dir):\n",
        "        \"\"\"\n",
        "        Delete all checkpoints in directory.\n",
        "        \"\"\"\n",
        "        filelist = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth.tar\")]\n",
        "        for f in filelist:\n",
        "            os.remove(os.path.join(checkpoint_dir, f))\n",
        "\n",
        "        print(\"Checkpoint successfully removed\")\n",
        "\n",
        "    class FROGDataset:\n",
        "        \"\"\"\n",
        "        Dog Dataset.\n",
        "        \"\"\"\n",
        "        def __init__(self, batch_size=4):\n",
        "            self.batch_size = batch_size\n",
        "            self.train_dataset,self.train_label,self.val_dataset,self.val_label = self.get_train_numpy()\n",
        "            self.train_loader,self.val_loader = self.get_dataloaders()\n",
        "\n",
        "        def get_train_numpy(self):\n",
        "            df = pd.read_pickle('/content/drive/MyDrive/Test_domain_generalization/Pyret_Labels_for_CNN_duration_2000_bin1_%02d.pkl' % (p + 5))\n",
        "\n",
        "            self.train_dataset = np.tile(np.expand_dims(df.iloc[0,1][:400,:,:],axis = 1),[1,3,1,1])\n",
        "            self.train_label = df.iloc[0,0][:400,:]\n",
        "            self.val_dataset = np.tile(np.expand_dims(df.iloc[0,1][:400,:,:],axis = 1),[1,3,1,1])\n",
        "            self.val_label = df.iloc[0,0][:400,:]\n",
        "            return self.train_dataset,self.train_label,self.val_dataset,self.val_label\n",
        "         \n",
        "        def get_dataloaders(self):\n",
        "            # train set\n",
        "            self.train_dataset = self.train_dataset.astype(np.double)\n",
        "            train_label = self.train_label \n",
        "            self.train_label  = self.train_label.astype(np.double)\n",
        "            train_set = torch.tensor(self.train_dataset)\n",
        "            train_label = torch.tensor(self.train_label)\n",
        "            dataset = torch.utils.data.TensorDataset(train_set, train_label)\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            print(type(train_loader))\n",
        "            # validation set\n",
        "            self.val_dataset  = self.val_dataset.astype(np.double)\n",
        "            self.val_label = self.val_label.astype(np.double)\n",
        "            val_set = torch.tensor(self.val_dataset)\n",
        "            val_label = torch.tensor(self.val_label)\n",
        "            datasetval = torch.utils.data.TensorDataset(val_set, val_label)\n",
        "            val_loader = torch.utils.data.DataLoader(datasetval, batch_size=self.batch_size, shuffle=False)\n",
        "            return train_loader,val_loader\n",
        "\n",
        "    dataset = FROGDataset()\n",
        "\n",
        "    def evaluate_loop(data_loader, model, criterion):\n",
        "        model.eval() \n",
        "        y_true, y_pred, running_loss = [], [], []\n",
        "        n=0\n",
        "        for X, y in data_loader:  \n",
        "            with torch.no_grad():\n",
        "                output = model(X)  # output is torch tensor\n",
        "                y_true.append(y)\n",
        "                y_pred.append(output)\n",
        "                n += 1\n",
        "                if criterion is not None:\n",
        "                    running_loss.append(criterion(output, y).item() * X.size(0))  # The output dimension: [batchsize,num of classes]\n",
        "        model.train() \n",
        "        y_true, y_pred = torch.cat(y_true), torch.cat(y_pred) # convert y_true from list to torch tensor\n",
        "        return y_true, y_pred, running_loss,n,output,y\n",
        "\n",
        "    def restore_checkpoint(model, checkpoint_dir, cuda=False, force=False, pretrain=False):\n",
        "        \"\"\"\n",
        "        If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
        "        Returns the model, the current epoch, and training losses.\n",
        "        \"\"\"\n",
        "        def get_epoch(cp):\n",
        "            return int(cp.split('epoch=')[-1].split('.checkpoint.pth.tar')[0])\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
        "                    if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
        "        cp_files.sort(key=lambda x: get_epoch(x))\n",
        "\n",
        "        if not cp_files:\n",
        "            print('No saved model parameters found')\n",
        "            if force:\n",
        "                raise Exception('Checkpoint not found')\n",
        "            else:\n",
        "                return model, 0, []\n",
        "        epochs = [get_epoch(cp) for cp in cp_files]\n",
        "\n",
        "        if not force:\n",
        "            epochs = [0] + epochs\n",
        "            print('Which epoch to load from? Choose from epochs below:')\n",
        "            print(epochs)\n",
        "            print('Enter 0 to train from scratch.')\n",
        "            print(\">> \", end='')\n",
        "            inp_epoch = int(input())\n",
        "            if inp_epoch not in epochs:\n",
        "                raise Exception(\"Invalid epoch number\")\n",
        "            if inp_epoch == 0:\n",
        "                print(\"Checkpoint not loaded\")\n",
        "                clear_checkpoint(checkpoint_dir)\n",
        "                return model, 0, []\n",
        "        else:\n",
        "            print('Which epoch to load from? Choose from epochs below:')\n",
        "            print(epochs)\n",
        "            print(\">> \", end='')\n",
        "            inp_epoch = int(input())\n",
        "            if inp_epoch not in epochs:\n",
        "                raise Exception(\"Invalid epoch number\")\n",
        "\n",
        "        filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
        "\n",
        "        print(\"Loading from checkpoint {}\".format(filename))\n",
        "\n",
        "        if cuda:\n",
        "            checkpoint = torch.load(filename)\n",
        "        else:\n",
        "            checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "\n",
        "        try:\n",
        "            stats = checkpoint['stats']\n",
        "            if pretrain:\n",
        "                model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> Successfully restored checkpoint (trained for {} epochs)\".format(checkpoint['epoch']))\n",
        "        except:\n",
        "            print(\"=> Checkpoint not successfully restored\")\n",
        "            raise\n",
        "\n",
        "        return model, inp_epoch, stats\n",
        "\n",
        "    def train(config, dataset, model):\n",
        "        \"\"\"\n",
        "        restore the checkpoint of the base model, and evaluate its accuracy on different classes (different duration) of pulses\n",
        "        \"\"\"\n",
        "        # p = 0\n",
        "        train_loader, val_loader = dataset.train_loader, dataset.val_loader \n",
        "        if 'use_weighted' not in config:\n",
        "            criterion = torch.nn.MSELoss() # Here we diefine the criteron for the evaluate \n",
        "            #epoch and evaluate_loop\n",
        "        else:\n",
        "            criterion = torch.nn.MSELoss() # how to define weight?\n",
        "        print('Loading model...') \n",
        "        force = config['ckpt_force'] if 'ckpt_force' in config else False\n",
        "        model, start_epoch, stats = restore_checkpoint(model, config['ckpt_path'], force=force)\n",
        "        y_true, y_pred, running_loss,n,output,y = evaluate_loop(val_loader, model, criterion)\n",
        "\n",
        "        print('Finished Training')\n",
        "        return np.sum(running_loss)/len(running_loss),y_true, y_pred, model\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        config = {\n",
        "            'batch_size': 4,\n",
        "            'ckpt_path': '/content/drive/MyDrive/checkpointsResNet4e-4_120_24_duration_5_15_2fc_long/transfer',\n",
        "            'plot_name': 'TransferCPU_4_4e-3_50EP',\n",
        "            'dict_path': '/content/drive/MyDrive/ResNet18.pt',\n",
        "\n",
        "        }\n",
        "        model = load_pretrained(config['dict_path'])\n",
        "        lin = model.fc\n",
        "        new_lin = nn.Sequential(\n",
        "            lin,\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(lin.out_features, 1000),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1000, 128)\n",
        "        )\n",
        "        model.fc = new_lin\n",
        "        model = model.double()\n",
        "        Loss,y_true, y_pred,model = train(config, dataset, model)  \n",
        "        model = model.cuda()\n",
        "        print(\"The loss of the base model on class %02d is: \" % (p+5), Loss)\n",
        "    ############################\n",
        "    \"\"\"\n",
        "    Freeze all the parameters except for the fc block by setting the requires_grad to False, train the fc block using different classes of pulses\n",
        "    \"\"\"\n",
        "    cntr = 0\n",
        "    lt = 10\n",
        "    for child in model.children():\n",
        "        cntr+=1\n",
        "        if cntr < lt:\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "    #############################\n",
        "\n",
        "\n",
        "    def save_checkpoint(model, epoch, checkpoint_dir, stats):\n",
        "        \"\"\"\n",
        "        Save model checkpoint.\n",
        "        \"\"\"\n",
        "        state = {\n",
        "            'epoch': epoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'stats': stats,\n",
        "        }\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        filename = os.path.join(checkpoint_dir,'epoch={}.checkpoint.pth.tar'.format(epoch))\n",
        "        torch.save(state, filename)\n",
        "        \n",
        "    def restore_checkpoint(model, checkpoint_dir, cuda=True, force=False, pretrain=True):\n",
        "        \"\"\"\n",
        "        If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
        "        Returns the model, the current epoch, and training losses.\n",
        "        \"\"\"\n",
        "        def get_epoch(cp):\n",
        "            return int(cp.split('epoch=')[-1].split('.checkpoint.pth.tar')[0])\n",
        "\n",
        "        if not os.path.exists(checkpoint_dir):\n",
        "            os.makedirs(checkpoint_dir)\n",
        "        cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
        "                    if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
        "        cp_files.sort(key=lambda x: get_epoch(x))\n",
        "\n",
        "        if not cp_files:\n",
        "            print('No saved model parameters found')\n",
        "            if force:\n",
        "                raise Exception('Checkpoint not found')\n",
        "            else:\n",
        "                return model, 0, []\n",
        "\n",
        "        # Find latest epoch\n",
        "        epochs = [get_epoch(cp) for cp in cp_files]\n",
        "\n",
        "        if not force:\n",
        "            epochs = [0] + epochs\n",
        "            print('Which epoch to load from? Choose from epochs below:')\n",
        "            print(epochs)\n",
        "            print('Enter 0 to train from scratch.')\n",
        "            print(\">> \", end='')\n",
        "            inp_epoch = 0\n",
        "            #inp_epoch = int(input())\n",
        "            if inp_epoch not in epochs:\n",
        "                raise Exception(\"Invalid epoch number\")\n",
        "            if inp_epoch == 0:\n",
        "                print(\"Checkpoint not loaded\")\n",
        "                clear_checkpoint(checkpoint_dir)\n",
        "                return model, 0, []\n",
        "        else:\n",
        "            print('Which epoch to load from? Choose from epochs below:')\n",
        "            print(epochs)\n",
        "            print(\">> \", end='')\n",
        "            inp_epoch = int(input())\n",
        "            if inp_epoch not in epochs:\n",
        "                raise Exception(\"Invalid epoch number\")\n",
        "\n",
        "        filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
        "\n",
        "        print(\"Loading from checkpoint {}\".format(filename))\n",
        "\n",
        "        if cuda:\n",
        "            checkpoint = torch.load(filename)\n",
        "        else:\n",
        "            # Load GPU model on CPU\n",
        "            checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "\n",
        "        try:\n",
        "            stats = checkpoint['stats']\n",
        "            if pretrain:\n",
        "                model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "            else:\n",
        "                model.load_state_dict(checkpoint['state_dict'])\n",
        "            print(\"=> Successfully restored checkpoint (trained for {} epochs)\".format(checkpoint['epoch']))\n",
        "        except:\n",
        "            print(\"=> Checkpoint not successfully restored\")\n",
        "            raise\n",
        "\n",
        "        return model, inp_epoch, stats\n",
        "\n",
        "    class Plotter:\n",
        "        def __init__(self, stats=[], name='CNN'):\n",
        "            self.stats = stats\n",
        "            self.name = name\n",
        "            self.axes = self.make_cnn_training_plot()\n",
        "\n",
        "        def make_cnn_training_plot(self):\n",
        "            \"\"\"\n",
        "            Runs the setup for an interactive matplotlib graph that logs the loss and accuracy\n",
        "            \"\"\"\n",
        "            print('Setting up interactive graph...')\n",
        "            plt.ion()\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "            plt.suptitle(self.name + ' Training')\n",
        "            axes[0].set_xlabel('Epoch')\n",
        "            axes[0].set_ylabel('Accuracy')\n",
        "            axes[1].set_xlabel('Epoch')\n",
        "            axes[1].set_ylabel('Loss')\n",
        "            return axes\n",
        "\n",
        "        def log_cnn_training(self, epoch):\n",
        "            \"\"\"\n",
        "            Logs the validation accuracy and loss to the terminal\n",
        "            \"\"\"\n",
        "            valid_loss, train_loss = self.stats[-1]\n",
        "            print('Epoch {}'.format(epoch))\n",
        "            print('\\tValidation Loss: {}'.format(valid_loss))\n",
        "            print('\\tTrain Loss: {}'.format(train_loss))\n",
        "\n",
        "        def update_cnn_training_plot(self, epoch):\n",
        "            \"\"\"\n",
        "            Updates the training plot with a new data point for loss and accuracy\n",
        "            \"\"\"\n",
        "            xrange = range(epoch - len(self.stats) + 1, epoch + 1)\n",
        "\n",
        "            self.axes[1].plot(xrange, [s[0] for s in self.stats], linestyle='--', marker='o', color='b')\n",
        "            self.axes[1].plot(xrange, [s[1] for s in self.stats], linestyle='--', marker='o', color='r')\n",
        "            self.axes[0].legend(['Validation', 'Train'])\n",
        "            self.axes[1].legend(['Validation', 'Train'])\n",
        "            plt.pause(0.00001)\n",
        "\n",
        "        def save_cnn_training_plot(self):\n",
        "            \"\"\"\n",
        "            Saves the training plot to a file\n",
        "            \"\"\"\n",
        "            plt.savefig(self.name + '_training_plot.png', dpi=200)\n",
        "\n",
        "        def hold_training_plot(self):\n",
        "            \"\"\"\n",
        "            Keep the program alive to display the training plot\n",
        "            \"\"\"\n",
        "            plt.ioff()\n",
        "            plt.show()\n",
        "    class FROGDataset:\n",
        "        \"\"\"\n",
        "        Dog Dataset.\n",
        "        \"\"\"\n",
        "        def __init__(self, batch_size=4):\n",
        "            self.batch_size = batch_size\n",
        "            self.train_dataset,self.train_label,self.val_dataset,self.val_label = self.get_train_numpy()\n",
        "            self.train_loader,self.val_loader = self.get_dataloaders()\n",
        "\n",
        "        def get_train_numpy(self):\n",
        "            df = pd.read_pickle('/content/drive/MyDrive/Test_domain_generalization/Pyret_Labels_for_CNN_duration_2000_bin1_%02d.pkl' % (p + 5))\n",
        "\n",
        "            self.train_dataset = np.tile(np.expand_dims(df.iloc[0,1][:1800,:,:],axis = 1),[1,3,1,1])\n",
        "            self.train_label = df.iloc[0,0][:1800,:]\n",
        "            self.val_dataset = np.tile(np.expand_dims(df.iloc[0,1][1800:,:,:],axis = 1),[1,3,1,1])\n",
        "            self.val_label = df.iloc[0,0][1800:,:]\n",
        "            return self.train_dataset,self.train_label,self.val_dataset,self.val_label\n",
        "\n",
        "        def get_dataloaders(self):\n",
        "            # train set\n",
        "            self.train_dataset = self.train_dataset.astype(np.double)\n",
        "            train_label = self.train_label \n",
        "            self.train_label  = self.train_label.astype(np.double)\n",
        "            train_set = torch.tensor(self.train_dataset)\n",
        "            train_label = torch.tensor(self.train_label)\n",
        "            dataset = torch.utils.data.TensorDataset(train_set, train_label)\n",
        "            train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=False)\n",
        "            print(type(train_loader))\n",
        "            # validation set\n",
        "            self.val_dataset  = self.val_dataset.astype(np.double)\n",
        "            self.val_label = self.val_label.astype(np.double)\n",
        "            val_set = torch.tensor(self.val_dataset)\n",
        "            val_label = torch.tensor(self.val_label)\n",
        "            datasetval = torch.utils.data.TensorDataset(val_set, val_label)\n",
        "            val_loader = torch.utils.data.DataLoader(datasetval, batch_size=self.batch_size, shuffle=False)\n",
        "            return train_loader,val_loader\n",
        "\n",
        "    def _train_epoch(train_loader, model, criterion, optimizer):\n",
        "        for i, (X, y) in enumerate(train_loader):             \n",
        "            # clear parameter gradients\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad() \n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)#. https://stackoverflow.com/questions/50408673/how-to-build-an-autograd-compatible-pytorch-module-that-resizes-tensors-like-ima\n",
        "            loss.backward()  \n",
        "            optimizer.step() \n",
        "\n",
        "    def _evaluate_epoch( train_loader, val_loader, model, criterion, epoch,batch_size): \n",
        "        \"\"\"\n",
        "        Evaluates the model on the train and validation set.\n",
        "        \"\"\"\n",
        "        stat = []\n",
        "        Worst_Labels = np.zeros((1,128))\n",
        "        Worst_FROG = np.zeros((1,3,64,64))\n",
        "        for data_loader in [val_loader, train_loader]:\n",
        "            if data_loader == val_loader:\n",
        "                y_true, y_pred, running_loss1,BatchNum_for_evaluate_epoch = evaluate_loop(data_loader, model, criterion) \n",
        "                total_loss = np.sum(running_loss1) / BatchNum_for_evaluate_epoch  \n",
        "                stat += [total_loss]\n",
        "            else:\n",
        "                y_true, y_pred, running_loss,BatchNum_for_evaluate_epoch = evaluate_loop(data_loader, model, criterion) \n",
        "                Indices_Worst_FROG = np.asarray(running_loss).argsort()[-int(np.floor(600/batch_size)):]\n",
        "                Loss_Worst = np.sum(np.take(running_loss,np.ndarray.tolist( Indices_Worst_FROG)))\n",
        "                total_loss = np.sum(running_loss) / BatchNum_for_evaluate_epoch   \n",
        "                stat += [total_loss]\n",
        "        return running_loss1, stat\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    def evaluate_loop(data_loader, model, criterion):\n",
        "        model.eval() \n",
        "        y_true, y_pred, running_loss = [], [], []\n",
        "        BatchNum_for_evaluate_epoch=0\n",
        "        for X, y in data_loader:  \n",
        "            with torch.no_grad():\n",
        "                output = model(X)  # output is torch tensor\n",
        "                y_true.append(y)\n",
        "                y_pred.append(output)\n",
        "                BatchNum_for_evaluate_epoch += 1\n",
        "                if criterion is not None:\n",
        "                    running = []\n",
        "                    for i in range(0,output.size()[0]):\n",
        "\n",
        "                        running.append(criterion(output[i,:], y[i,:]).item() )  # The output [batchsize,num of classes]\n",
        "                    running_loss += running\n",
        "        model.train() \n",
        "        return y_true, y_pred, running_loss,BatchNum_for_evaluate_epoch\n",
        "\n",
        "    def load_pretrained(dict_path):\n",
        "        \"\"\"\n",
        "        Load a ResNet-18 model from `torchvision.models` with pre-trained weights. Freeze all the parameters besides the\n",
        "        final layer by setting the flag `requires_grad` for each parameter to False. Replace the final fully connected layer\n",
        "        with another fully connected layer with `num_classes` many output units.\n",
        "        Inputs:\n",
        "            - num_classes: int\n",
        "        Returns:\n",
        "            - model: PyTorch model\n",
        "        \"\"\"\n",
        "        # TODO (part d): load a pre-trained ResNet-18 model\n",
        "        Resnet18 = models.resnet18(pretrained=False)\n",
        "        if (dict_path != None):\n",
        "            Resnet18.load_state_dict(torch.load(dict_path))\n",
        "        for param in Resnet18.parameters():\n",
        "            param.requires_grad = True\n",
        "        n_inputs = Resnet18.fc.in_features\n",
        "        Resnet18.fc = torch.nn.Linear(512,1000)\n",
        "        return Resnet18\n",
        "\n",
        "    def initialize_weights(net):\n",
        "        for m in net.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.weight.data.normal_(0, 0.02)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.ConvTranspose2d):\n",
        "                m.weight.data.normal_(0, 0.02)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.02)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def print_network(net):\n",
        "        num_params = 0\n",
        "        for param in net.parameters():\n",
        "            num_params += param.numel()\n",
        "        print(net)\n",
        "        print('Total number of parameters: %d' % num_params)\n",
        "\n",
        "    def imsave1(X,epoch, edge):\n",
        "        for i in range( 0,edge* edge):\n",
        "            plt.subplot(edge, edge,  i+1)\n",
        "            plt.axis('off')\n",
        "            plt.plot(X[i,0,:])\n",
        "        plt.subplot(edge, edge,  1)\n",
        "        filename1 = 'continouse_generated_plot_%02d.png' % (epoch+1)\n",
        "        plt.savefig(filename1)\n",
        "        plt.close()\n",
        "\n",
        "    def train(config, dataset, model):\n",
        "        train_loader, val_loader = dataset.train_loader, dataset.val_loader \n",
        "\n",
        "        if 'use_weighted' not in config:\n",
        "            criterion = torch.nn.MSELoss() # Here we diefine the criteron for the evaluate \n",
        "        else:\n",
        "            criterion = torch.nn.MSELoss() \n",
        "        \n",
        "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr = config['learning_rate'])\n",
        "        # Attempts to restore the latest checkpoint if exists\n",
        "        print('Loading model...')\n",
        "        Train_losses, Statt = [],[]\n",
        "        for epoch in range(0, config['num_epoch'],2):\n",
        "            # Train model on training set\n",
        "            time1 = time.time()\n",
        "            _train_epoch(train_loader, model, criterion, optimizer)\n",
        "            time2 = time.time()\n",
        "            print(\"time_train_epoch\",time2 -time1)\n",
        "            # Evaluate model on training and validation set\n",
        "            critetion1 = torch.nn.L1Loss()\n",
        "            Train_loss, Stat = _evaluate_epoch( train_loader, val_loader, model, criterion1, epoch + 1,config['batch_size'])\n",
        "            time3 = time.time()\n",
        "            print(\"time_evaluate_epoch\",time3 -time2)\n",
        "            Train_losses.append(Train_loss)\n",
        "            Statt.append(Stat)\n",
        "            save_checkpoint(model, epoch + 1, config['ckpt_save_path'], Stat)\n",
        "        return Train_losses,Statt\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        config = {\n",
        "            'batch_size': 24,\n",
        "            'ckpt_save_path': '/content/drive/MyDrive/checkpointsResNet4e-4_120_24_%02d_fine_tune_2fc_bin1/transfer' % (p + 5),\n",
        "            'plot_name': 'Transfer_4e-2_CNN',\n",
        "            'num_epoch': 80,\n",
        "            'learning_rate': 4e-5\n",
        "        }\n",
        "        dataset = FROGDataset(config['batch_size'])\n",
        "        Train_losses, Statt = train(config, dataset, model)\n",
        "        TotalLoss = pd.DataFrame(columns = ['Train_losses','Statt'])\n",
        "        TotalLoss.loc[0,'Train_losses'] = Train_losses\n",
        "        TotalLoss.loc[0,'Statt'] = Statt\n",
        "        if not os.path.exists('/content/drive/MyDrive/FROG training result/Loss/CNN'):\n",
        "            os.makedirs('/content/drive/MyDrive/FROG training result/Loss/CNN')\n",
        "        TotalLoss.to_pickle(\"/content/drive/MyDrive/FROG training result/Loss/CNN/TotalLoss_CNN_4e-4_120_24_%02d_fine_tune_2c_bin1.pkl\" % (p + 5))\n",
        "\n",
        "\n",
        "    #############################\n",
        "    #### loss plots for all the models. Validate the general improvement\n",
        "    fine_tune = pd.read_pickle('/content/drive/MyDrive/FROG training result/Loss/CNN/TotalLoss_CNN_4e-4_120_24_%02d_fine_tune_2c_bin1.pkl' % (p+5))\n",
        "    plt.plot(np.array(fine_tune['Statt'][0])[:,1])\n",
        "    #############################\n",
        "\n",
        "    \n",
        "    def load_pretrained(dict_path):\n",
        "        \"\"\"\n",
        "        Load a ResNet-18 model from `torchvision.models` without pre-trained weights\n",
        "        \"\"\"\n",
        "        Resnet18 = models.resnet18(pretrained=False)\n",
        "        if (dict_path != None):\n",
        "            Resnet18.load_state_dict(torch.load(dict_path))\n",
        "        for param in Resnet18.parameters():\n",
        "            param.requires_grad = True\n",
        "        n_inputs = Resnet18.fc.in_features\n",
        "        Resnet18.fc = torch.nn.Linear(512,1000)\n",
        "        return Resnet18\n",
        "\n",
        "    def evaluate_loop(data_loader, model, criterion):\n",
        "        model.eval() \n",
        "        y_true, y_pred, running_loss = [], [], []\n",
        "        n=0\n",
        "        for X, y in data_loader: \n",
        "            with torch.no_grad():\n",
        "                output = model(X) \n",
        "                y_true.append(y)\n",
        "                y_pred.append(output)\n",
        "                n += 1\n",
        "                if criterion is not None:\n",
        "                    running_loss.append(criterion(output, y).item() )  # The output [batchsize,num of classes]\n",
        "        model.train() \n",
        "        y_true, y_pred = torch.cat(y_true), torch.cat(y_pred) \n",
        "        return y_true, y_pred, running_loss,n,output,y\n",
        "\n",
        "    def train(config, dataset, model):\n",
        "        \"\"\"\n",
        "        The dataset was already defined by the FROGDataset function earlier\n",
        "        \"\"\"\n",
        "        # Data loaders\n",
        "        train_loader, val_loader = dataset.train_loader, dataset.val_loader \n",
        "        if 'use_weighted' not in config:\n",
        "            criterion = torch.nn.L1Loss() \n",
        "        else:\n",
        "            criterion = torch.nn.L1Loss() \n",
        "        print('Loading model...') \n",
        "        force = config['ckpt_force'] if 'ckpt_force' in config else False\n",
        "        model, start_epoch, stats = restore_checkpoint(model, config['ckpt_path'], force=force)\n",
        "        y_true, y_pred, running_loss,n,output,y = evaluate_loop(val_loader, model, criterion)\n",
        "        print('Finished Training')\n",
        "        return np.sum(running_loss)/len(running_loss),y_true, y_pred, model\n",
        "\n",
        "    if __name__ == '__main__':\n",
        "        config = {\n",
        "            'batch_size': 4,\n",
        "            'ckpt_path': '/content/drive/MyDrive/checkpointsResNet4e-4_120_24_%02d_fine_tune_2fc_bin1/transfer' % (p + 5),\n",
        "            'plot_name': 'TransferCPU_4_4e-3_50EP',\n",
        "            'dict_path': '/content/drive/MyDrive/ResNet18.pt',\n",
        "\n",
        "        }\n",
        "        model = load_pretrained(config['dict_path'])\n",
        "        lin = model.fc\n",
        "        new_lin = nn.Sequential(\n",
        "            lin,\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(lin.out_features, 1000),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(1000, 128)\n",
        "        )\n",
        "        model.fc = new_lin\n",
        "        model = model.double()\n",
        "        model = model.eval()\n",
        "        Loss,y_true, y_pred,model = train(config, dataset, model)  \n",
        "        print(\"The loss of the model with fine-tuned fc block on class %02d is: \" % (p+5), Loss)\n",
        "        print(np.mean(np.sum(np.square(y_true.cpu().numpy() - y_pred.cpu().numpy()),axis = 1)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnxTLDNqZo-F"
      },
      "source": [
        "\"\"\"\n",
        "Concatenate 10 fine-tuned (feature extraction) FC blocks together \n",
        "\"\"\"\n",
        "\n",
        "def restore_checkpoint(model, checkpoint_dir, cuda=False, force=False, pretrain=False):\n",
        "    \"\"\"\n",
        "    If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
        "    Returns the model, the current epoch, and training losses.\n",
        "    \"\"\"\n",
        "    def get_epoch(cp):\n",
        "        return int(cp.split('epoch=')[-1].split('.checkpoint.pth.tar')[0])\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
        "                if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
        "    cp_files.sort(key=lambda x: get_epoch(x))\n",
        "\n",
        "    if not cp_files:\n",
        "        print('No saved model parameters found')\n",
        "        if force:\n",
        "            raise Exception('Checkpoint not found')\n",
        "        else:\n",
        "            return model, 0, []\n",
        "\n",
        "    # Find latest epoch\n",
        "    epochs = [get_epoch(cp) for cp in cp_files]\n",
        "\n",
        "    if not force:\n",
        "        epochs = [0] + epochs\n",
        "        print('Which epoch to load from? Choose from epochs below:')\n",
        "        print(epochs)\n",
        "        print('Enter 0 to train from scratch.')\n",
        "        print(\">> \", end='')\n",
        "        inp_epoch = int(input())\n",
        "        if inp_epoch not in epochs:\n",
        "            raise Exception(\"Invalid epoch number\")\n",
        "        if inp_epoch == 0:\n",
        "            print(\"Checkpoint not loaded\")\n",
        "            clear_checkpoint(checkpoint_dir)\n",
        "            return model, 0, []\n",
        "    else:\n",
        "        print('Which epoch to load from? Choose from epochs below:')\n",
        "        print(epochs)\n",
        "        print(\">> \", end='')\n",
        "        inp_epoch = int(input())\n",
        "        if inp_epoch not in epochs:\n",
        "            raise Exception(\"Invalid epoch number\")\n",
        "\n",
        "    filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
        "\n",
        "    print(\"Loading from checkpoint {}\".format(filename))\n",
        "\n",
        "    if cuda:\n",
        "        checkpoint = torch.load(filename)\n",
        "    else:\n",
        "        # Load GPU model on CPU\n",
        "        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    try:\n",
        "        stats = checkpoint['stats']\n",
        "        if pretrain:\n",
        "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "        print(\"=> Successfully restored checkpoint (trained for {} epochs)\".format(checkpoint['epoch']))\n",
        "    except:\n",
        "        print(\"=> Checkpoint not successfully restored\")\n",
        "        raise\n",
        "\n",
        "    return model, inp_epoch, stats\n",
        "#### Use N_bins to define the number of bins (N_bins = 10 here)\n",
        "N_bins = 10\n",
        "fc_layers1_weight = torch.zeros((N_bins,1000,512))\n",
        "fc_layers2_weight = torch.zeros((N_bins,1000,1000))\n",
        "fc_layers3_weight = torch.zeros((N_bins,128,1000))\n",
        "fc_layers1_bias = torch.zeros((N_bins,1000))\n",
        "fc_layers2_bias = torch.zeros((N_bins,1000))\n",
        "fc_layers3_bias = torch.zeros((N_bins,128))\n",
        "for p in range(0,N_bins):\n",
        "\n",
        "    config = {\n",
        "            'batch_size': 24,\n",
        "            'ckpt_path': '/content/drive/MyDrive/checkpointsResNet4e-4_120_24_%02d_fine_tune_2fc_bin1/transfer' % (p + 5),\n",
        "            'plot_name': 'Transfer_4e-2_CNN',\n",
        "            'dict_path': '/content/drive/MyDrive/ResNet18.pt',\n",
        "            'num_epoch': 40,\n",
        "            'learning_rate': 4e-4,\n",
        "    }\n",
        "    model = load_pretrained(config['dict_path'])\n",
        "    lin = model.fc\n",
        "    new_lin = nn.Sequential(\n",
        "        lin,\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(lin.out_features, 1000),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(1000, 128),\n",
        "    )\n",
        "    model.fc = new_lin\n",
        "    model = model.double()\n",
        "    print('Loading model...') \n",
        "    force = config['ckpt_force'] if 'ckpt_force' in config else False\n",
        "    model, start_epoch, stats = restore_checkpoint(model, config['ckpt_path'], force=force)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.cuda()\n",
        "    fc_layers1_weight[p,:,:] =  model.fc[0].weight\n",
        "    fc_layers2_weight[p,:,:] =  model.fc[2].weight\n",
        "    fc_layers3_weight[p,:,:] =  model.fc[4].weight\n",
        "    fc_layers1_bias[p,:] =  model.fc[0].bias\n",
        "    fc_layers2_bias[p,:] =  model.fc[2].bias\n",
        "    fc_layers3_bias[p,:] =  model.fc[4].bias\n",
        "fc_layers = pd.DataFrame(columns=[\"fc_layers1_weight\",\"fc_layers2_weight\",\"fc_layers3_weight\",\"fc_layers1_bias\",\"fc_layers2_bias\",\"fc_layers3_bias\"])\n",
        "fc_layers['fc_layers1_weight'] = fc_layers['fc_layers1_weight'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers1_weight'] = fc_layers1_weight.cpu().detach()\n",
        "fc_layers['fc_layers2_weight'] = fc_layers['fc_layers2_weight'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers2_weight'] = fc_layers2_weight.cpu().detach()\n",
        "fc_layers['fc_layers3_weight'] = fc_layers['fc_layers3_weight'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers3_weight'] = fc_layers3_weight.cpu().detach()\n",
        "fc_layers['fc_layers1_bias'] = fc_layers['fc_layers1_bias'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers1_bias'] = fc_layers1_bias.cpu().detach()\n",
        "fc_layers['fc_layers2_bias'] = fc_layers['fc_layers2_bias'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers2_bias'] = fc_layers2_bias.cpu().detach()\n",
        "fc_layers['fc_layers3_bias'] = fc_layers['fc_layers3_bias'].astype(object)\n",
        "fc_layers.loc[0, 'fc_layers3_bias'] = fc_layers3_bias.cpu().detach()\n",
        "fc_layers.to_pickle('/content/drive/MyDrive/Test_domain_generalization/fc_layers.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFmkMBqpcJrd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}