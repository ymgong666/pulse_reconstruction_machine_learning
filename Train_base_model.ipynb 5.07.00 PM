{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_base_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNh2KvNuvbnF"
      },
      "source": [
        "\"\"\"\n",
        "This code trains a ResNet model from scratch on pulses with a variaty of FWHMs (25fs ~ 75fs)\n",
        "\"\"\"\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from matplotlib import pyplot\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import argparse\n",
        "import  torch, time, os, pickle, itertools\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import time\n",
        "%matplotlib inline\n",
        "\n",
        "def save_checkpoint(model, epoch, checkpoint_dir, stats):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'stats': stats,\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    filename = os.path.join(checkpoint_dir,'epoch={}.checkpoint.pth.tar'.format(epoch))\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def save_checkpoint_for_GAN(model, epoch, checkpoint_dir):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "    \"\"\"\n",
        "    state = {\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "    }\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    filename = os.path.join(checkpoint_dir,'epoch={}.checkpoint.pth.tar'.format(epoch))\n",
        "    torch.save(state, filename)\n",
        "\n",
        "def restore_checkpoint(model, checkpoint_dir, cuda=True, force=False, pretrain=True):\n",
        "    \"\"\"\n",
        "    If a checkpoint exists, restores the PyTorch model from the checkpoint.\n",
        "    Returns the model, the current epoch, and training losses.\n",
        "    \"\"\"\n",
        "    def get_epoch(cp):\n",
        "        return int(cp.split('epoch=')[-1].split('.checkpoint.pth.tar')[0])\n",
        "\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "    cp_files = [file_ for file_ in os.listdir(checkpoint_dir)\n",
        "                if file_.startswith('epoch=') and file_.endswith('.checkpoint.pth.tar')]\n",
        "    cp_files.sort(key=lambda x: get_epoch(x))\n",
        "\n",
        "    if not cp_files:\n",
        "        print('No saved model parameters found')\n",
        "        if force:\n",
        "            raise Exception('Checkpoint not found')\n",
        "        else:\n",
        "            return model, 0, []\n",
        "\n",
        "    # Find latest epoch\n",
        "    epochs = [get_epoch(cp) for cp in cp_files]\n",
        "\n",
        "    if not force:\n",
        "        epochs = [0] + epochs\n",
        "        print('Which epoch to load from? Choose from epochs below:')\n",
        "        print(epochs)\n",
        "        print('Enter 0 to train from scratch.')\n",
        "        print(\">> \", end='')\n",
        "        inp_epoch = 0\n",
        "        if inp_epoch not in epochs:\n",
        "            raise Exception(\"Invalid epoch number\")\n",
        "        if inp_epoch == 0:\n",
        "            print(\"Checkpoint not loaded\")\n",
        "            clear_checkpoint(checkpoint_dir)\n",
        "            return model, 0, []\n",
        "    else:\n",
        "        print('Which epoch to load from? Choose from epochs below:')\n",
        "        print(epochs)\n",
        "        print(\">> \", end='')\n",
        "        inp_epoch = int(input())\n",
        "        if inp_epoch not in epochs:\n",
        "            raise Exception(\"Invalid epoch number\")\n",
        "\n",
        "    filename = os.path.join(checkpoint_dir, 'epoch={}.checkpoint.pth.tar'.format(inp_epoch))\n",
        "\n",
        "    print(\"Loading from checkpoint {}\".format(filename))\n",
        "\n",
        "    if cuda:\n",
        "        checkpoint = torch.load(filename)\n",
        "    else:\n",
        "        checkpoint = torch.load(filename, map_location=lambda storage, loc: storage)\n",
        "\n",
        "    try:\n",
        "        stats = checkpoint['stats']\n",
        "        if pretrain:\n",
        "            model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint['state_dict'])\n",
        "        print(\"=> Successfully restored checkpoint (trained for {} epochs)\".format(checkpoint['epoch']))\n",
        "    except:\n",
        "        print(\"=> Checkpoint not successfully restored\")\n",
        "        raise\n",
        "\n",
        "    return model, inp_epoch, stats\n",
        "\n",
        "\n",
        "def clear_checkpoint(checkpoint_dir):\n",
        "    \"\"\"\n",
        "    Delete all checkpoints in directory.\n",
        "    \"\"\"\n",
        "    filelist = [f for f in os.listdir(checkpoint_dir) if f.endswith(\".pth.tar\")]\n",
        "    for f in filelist:\n",
        "        os.remove(os.path.join(checkpoint_dir, f))\n",
        "\n",
        "    print(\"Checkpoint successfully removed\")\n",
        "\n",
        "class Plotter:\n",
        "    def __init__(self, stats=[], name='CNN'):\n",
        "        self.stats = stats\n",
        "        self.name = name\n",
        "        self.axes = self.make_cnn_training_plot()\n",
        "\n",
        "    def make_cnn_training_plot(self):\n",
        "        \"\"\"\n",
        "        Runs the setup for an interactive matplotlib graph that logs the loss and accuracy\n",
        "        \"\"\"\n",
        "        print('Setting up interactive graph...')\n",
        "        plt.ion()\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "        plt.suptitle(self.name + ' Training')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].set_ylabel('Accuracy')\n",
        "        axes[1].set_xlabel('Epoch')\n",
        "        axes[1].set_ylabel('Loss')\n",
        "        return axes\n",
        "\n",
        "    def log_cnn_training(self, epoch):\n",
        "        \"\"\"\n",
        "        Logs the validation accuracy and loss to the terminal\n",
        "        \"\"\"\n",
        "        valid_loss, train_loss = self.stats[-1]\n",
        "        print('Epoch {}'.format(epoch))\n",
        "        print('\\tValidation Loss: {}'.format(valid_loss))\n",
        "#         print('\\tValidation Accuracy: {}'.format(valid_acc))\n",
        "        print('\\tTrain Loss: {}'.format(train_loss))\n",
        "#         print('\\tTrain Accuracy: {}'.format(train_acc))\n",
        "\n",
        "    def update_cnn_training_plot(self, epoch):\n",
        "        \"\"\"\n",
        "        Updates the training plot with a new data point for loss and accuracy\n",
        "        \"\"\"\n",
        "        xrange = range(epoch - len(self.stats) + 1, epoch + 1)\n",
        "        self.axes[1].plot(xrange, [s[0] for s in self.stats], linestyle='--', marker='o', color='b')\n",
        "        self.axes[1].plot(xrange, [s[1] for s in self.stats], linestyle='--', marker='o', color='r')\n",
        "        self.axes[0].legend(['Validation', 'Train'])\n",
        "        self.axes[1].legend(['Validation', 'Train'])\n",
        "        plt.pause(0.00001)\n",
        "\n",
        "    def save_cnn_training_plot(self):\n",
        "        \"\"\"\n",
        "        Saves the training plot to a file\n",
        "        \"\"\"\n",
        "        plt.savefig(self.name + '_training_plot.png', dpi=200)\n",
        "\n",
        "    def hold_training_plot(self):\n",
        "        \"\"\"\n",
        "        Keep the program alive to display the training plot\n",
        "        \"\"\"\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "\n",
        "####  dataset\n",
        "\n",
        "\n",
        "class FROGDataset:\n",
        "    \"\"\"\n",
        "    Dog Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.train_dataset,self.train_label,self.val_dataset,self.val_label = self.get_train_numpy()\n",
        "        self.train_loader,self.val_loader = self.get_dataloaders()\n",
        "\n",
        "    def get_train_numpy(self):\n",
        "        df = pd.read_pickle('/content/drive/MyDrive/Test_domain_generalization/Pyret_Labels_for_CNN_duration_10000.pkl')\n",
        "\n",
        "        self.train_dataset = np.tile(np.expand_dims(df.iloc[0,1][:10000,:,:],axis = 1),[1,3,1,1])\n",
        "        self.train_label = df.iloc[0,0][:10000,:]\n",
        "        df_val = pd.read_pickle('/content/drive/MyDrive/Test_domain_generalization/Pyret_Labels_for_CNN_duration_1000.pkl')\n",
        "\n",
        "        self.val_dataset = np.tile(np.expand_dims(df_val.iloc[0,1][:1000,:,:],axis = 1),[1,3,1,1])\n",
        "        self.val_label = df_val.iloc[0,0][:1000,:]\n",
        "        print('----',np.max(self.train_dataset[1,:,:]))\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(self.train_dataset[1,1,:,:])\n",
        "        return self.train_dataset,self.train_label,self.val_dataset,self.val_label\n",
        "    \n",
        "\n",
        "\n",
        "         \n",
        "    def get_dataloaders(self):\n",
        "        # train set\n",
        "        self.train_dataset = self.train_dataset.astype(np.double)\n",
        "        self.train_label  = self.train_label.astype(np.double)\n",
        "        \n",
        "        #self.train_dataset = (self.train_dataset-self.x_mean)/self.x_std\n",
        "        #train_label = (train_label-self.label_mean)/self.label_std\n",
        "        train_set = torch.tensor(self.train_dataset)\n",
        "        train_label = torch.tensor(self.train_label)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        train_set = train_set.to(device)\n",
        "        train_label = train_label.to(device)\n",
        "        \n",
        "        dataset = torch.utils.data.TensorDataset(train_set, train_label)\n",
        "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        # validation set\n",
        "        self.val_dataset  = self.val_dataset.astype(np.double)\n",
        "        self.val_label = self.val_label.astype(np.double)\n",
        "        val_set = torch.tensor(self.val_dataset)\n",
        "        val_label = torch.tensor(self.val_label)\n",
        "        val_set = val_set.to(device)\n",
        "        val_label = val_label.to(device)\n",
        "        datasetval = torch.utils.data.TensorDataset(val_set, val_label)\n",
        "        val_loader = torch.utils.data.DataLoader(datasetval, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "#         for i, (X,Y) in enumerate(train_loader): \n",
        "#             print(X.size())\n",
        "        return train_loader,val_loader\n",
        "    \n",
        "    def compensation_loader(self, N):\n",
        "        idx = np.random.randint(self.train_label.shape[0], size = N)\n",
        "        compensation_label = torch.tensor(self.train_label.astype(np.double)[idx,:])\n",
        "        compensation_dataset = torch.tensor(self.train_dataset.astype(np.double)[idx,:,:])\n",
        "        compensation_label = compensation_label.to(device)\n",
        "        compensation_dataset = compensation_dataset.to(device)\n",
        "        compensation = torch.utils.data.TensorDataset(compensation_dataset, compensation_label)\n",
        "        compensation_loader = torch.utils.data.DataLoader(compensation, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        return compensation_loader\n",
        "\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "def _train_epoch(train_loader, model, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    Train the model for one iteration through the train set.\n",
        "\n",
        "    \"\"\"\n",
        "    for i, (X, y) in enumerate(train_loader):   \n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad() \n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = model(X)\n",
        "        loss = criterion(output, y)#. https://stackoverflow.com/questions/50408673/how-to-build-an-autograd-compatible-pytorch-module-that-resizes-tensors-like-ima\n",
        "        loss.backward()  \n",
        "        optimizer.step() \n",
        "\n",
        "def _evaluate_epoch(plotter, train_loader, val_loader, model, criterion, epoch,batch_size): \n",
        "    \"\"\"\n",
        "    Evaluates the model on the train and validation set.\n",
        "    \"\"\"\n",
        "    stat = []\n",
        "    Worst_Labels = np.zeros((1,128))\n",
        "    Worst_FROG = np.zeros((1,3,64,64))\n",
        "    for data_loader in [val_loader, train_loader]:\n",
        "        if data_loader == val_loader:\n",
        "            y_true, y_pred, running_loss1,BatchNum_for_evaluate_epoch = evaluate_loop(data_loader, model, criterion) \n",
        "            total_loss = np.sum(running_loss1) / len(running_loss1)      \n",
        "            stat += [total_loss]\n",
        "        else:\n",
        "            y_true1, y_pred1, running_loss,BatchNum_for_evaluate_epoch = evaluate_loop(data_loader, model, criterion) \n",
        "            Indices_Worst_FROG = np.asarray(running_loss).argsort()[-int(np.floor(600/batch_size)):]\n",
        "            Loss_Worst = np.sum(np.take(running_loss,np.ndarray.tolist( Indices_Worst_FROG)))\n",
        "            total_loss = np.sum(running_loss) / len(running_loss)      \n",
        "            stat += [total_loss]\n",
        "    return running_loss1, stat, y_true, y_pred\n",
        "\n",
        "    \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "def evaluate_loop(data_loader, model, criterion):\n",
        "    model.eval() \n",
        "    y_true, y_pred, running_loss = [], [], []\n",
        "    BatchNum_for_evaluate_epoch=0\n",
        "    for X, y in data_loader:  \n",
        "        with torch.no_grad():\n",
        "            output = model(X)  # output is torch tensor\n",
        "            y_true.append(y)\n",
        "            y_pred.append(output)\n",
        "            BatchNum_for_evaluate_epoch += 1\n",
        "            if criterion is not None:\n",
        "                running = []\n",
        "                for i in range(0,output.size()[0]):\n",
        "\n",
        "                    running.append(criterion(output[i,:], y[i,:]).item() )  # The output [batchsize,num of classes]\n",
        "                running_loss += running\n",
        "    model.train() \n",
        "    y_true, y_pred = torch.cat(y_true), torch.cat(y_pred) # convert y_true from list to torch tensor\n",
        "    return y_true, y_pred, running_loss,BatchNum_for_evaluate_epoch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_pretrained(dict_path):\n",
        "    Resnet18 = models.resnet18(pretrained=False)\n",
        "    if (dict_path != None):\n",
        "        Resnet18.load_state_dict(torch.load(dict_path))\n",
        "    for param in Resnet18.parameters():\n",
        "        param.requires_grad = True\n",
        "    n_inputs = Resnet18.fc.in_features\n",
        "    print(n_inputs)\n",
        "    Resnet18.fc = torch.nn.Linear(512,1000)\n",
        "    return Resnet18\n",
        "\n",
        "def initialize_weights(net):\n",
        "    for m in net.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.ConvTranspose2d):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "        elif isinstance(m, nn.Linear):\n",
        "            m.weight.data.normal_(0, 0.02)\n",
        "            m.bias.data.zero_()\n",
        "\n",
        "def print_network(net):\n",
        "    num_params = 0\n",
        "    for param in net.parameters():\n",
        "        num_params += param.numel()\n",
        "    print(net)\n",
        "    print('Total number of parameters: %d' % num_params)\n",
        "def imsave1(X,epoch, edge):\n",
        "    for i in range( 0,edge* edge):\n",
        "        plt.subplot(edge, edge,  i+1)\n",
        "        plt.axis('off')\n",
        "        plt.plot(X[i,0,:])\n",
        "\t# save plot to file\n",
        "    plt.subplot(edge, edge,  1)\n",
        "    filename1 = 'continouse_generated_plot_%02d.png' % (epoch+1)\n",
        "    plt.savefig(filename1)\n",
        "    plt.close()\n",
        "\n",
        "def train(config, dataset, model):\n",
        "    # Data loaders\n",
        "    train_loader, val_loader = dataset.train_loader, dataset.val_loader \n",
        "    if 'use_weighted' not in config:\n",
        "        criterion = torch.nn.MSELoss() \n",
        "    else:\n",
        "        criterion = torch.nn.MSELoss() \n",
        "    \n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = config['learning_rate'])\n",
        "    print('Loading model...') \n",
        "    force = config['ckpt_force'] if 'ckpt_force' in config else False\n",
        "    model, start_epoch, stats = restore_checkpoint(model, config['ckpt_path'], force=force)\n",
        "\n",
        "    # Create plotter\n",
        "    plot_name = config['plot_name'] if 'plot_name' in config else 'CNN'\n",
        "    plotter = Plotter(stats, plot_name)\n",
        "\n",
        "    # Evaluate the model\n",
        "    Train_losses, Statt = [],[]\n",
        "    for epoch in range(0, config['num_epoch'],2):\n",
        "        # Train model on training set\n",
        "        time1 = time.time()\n",
        "        _train_epoch(train_loader, model, criterion, optimizer)\n",
        "        time2 = time.time()\n",
        "        print(\"time_train_epoch\",time2 -time1)\n",
        "        # Evaluate model on training and validation set\n",
        "        criterion1 = torch.nn.L1Loss()\n",
        "        Train_loss, Stat, y_true, y_pred = _evaluate_epoch(plotter, train_loader, val_loader, model, criterion1, epoch + 1,config['batch_size'])\n",
        "        time3 = time.time()\n",
        "        print(\"time_evaluate_epoch\",time3 -time2)\n",
        "        Train_losses.append(Train_loss)\n",
        "        Statt.append(Stat)\n",
        "        save_checkpoint(model, epoch + 1, config['ckpt_path'], plotter.stats)\n",
        "\n",
        "    return Train_losses,Statt, y_true, y_pred\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'batch_size': 24,\n",
        "        'ckpt_path': '/content/drive/MyDrive/checkpointsResNet4e-4_120_24_duration_5_15_2fc_long/transfer',\n",
        "        'plot_name': 'Transfer_4e-2_CNN',\n",
        "        'dict_path': '/content/drive/MyDrive/ResNet18.pt',\n",
        "        'num_epoch': 100,\n",
        "        'learning_rate': 4e-5,\n",
        "    }\n",
        "    dataset = FROGDataset(config['batch_size'])\n",
        "    model = load_pretrained(config['dict_path'])\n",
        "    lin = model.fc\n",
        "    new_lin = nn.Sequential(\n",
        "        lin,\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(lin.out_features, 1000),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(1000, 128),\n",
        "    )\n",
        "    model.fc = new_lin\n",
        "    model = model.double()\n",
        "    model.cuda()\n",
        "    Train_losses, Statt, y_true, y_pred = train(config, dataset, model)\n",
        "    TotalLoss = pd.DataFrame(columns = ['Train_losses','Statt'])\n",
        "    TotalLoss.loc[0,'Train_losses'] = Train_losses\n",
        "    TotalLoss.loc[0,'Statt'] = Statt\n",
        "    if not os.path.exists('/content/drive/MyDrive/FROG training result/Loss/CNN'):\n",
        "        os.makedirs('/content/drive/MyDrive/FROG training result/Loss/CNN')\n",
        "    TotalLoss.to_pickle(\"/content/drive/MyDrive/FROG training result/Loss/CNN/TotalLoss_CNN_4e-4_120_24_duration_5_15_2fc_long.pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}